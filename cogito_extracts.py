"""
This script generates a text file for a given task defined in the configuration table
[dbo].[ACONFIG_PROCESNG_RULES] and places the file in a network share

Usage: 
    Can be called directly from the command line or integrated into other Python modules.

Main Features:
1) Generates one or more text files using data from Clarity or Caboodle
2) Places the file(s) in the "Outbound" folder of a network share
3) Optionally, places a 2nd copy of the file(s), taken from the "Outbound" folder,
   to the "Archive" folder of a network share
4) Optionally, places a 3rd copy of the file(s), taken from the "Outbound" folder, 
   to a specified folder in a Google Cloud Platform (GCP) bucket.

Args:

    It accepts the following positional parameters below.

    conn_epic_bi_server  (str)     : Name of the BI SQL Server where the BI database
                                     resides
    conn_epic_bi_db      (str)     : Name of the BI database where the configuration
                                     table "[dbo].[ACONFIG_PROCESNG_RULES]" resides
    task_name            (str)     : Name of the task to process
    task_proc_step_id    (integer) : Numeric Task Process ID generated by the SSIS 
                                     package that invokes this script. Such ID is used
                                     for updating auditing records in the BI database.
    copy_files_to_archive (bool)   : If True, copies the generated file to the "Archive"
                                        folder in the network share. If False, does not.
    copy_files_to_gcp_bucket (bool): If True, copies the generated file to a GCP bucket. 
                                        If False, does not.
"""
import argparse
# import logging
import string
import sys
import os
import io
from datetime import datetime,date, timedelta
from enum import Enum
import re
from typing import Tuple
from typing import List
import shutil
import csv
from google_crc32c import exc
import pyodbc
# to split large text files into smaller ones
from cogito_utils_file_ops import calc_seq_index
# from cogito_utils_file_ops import split_csv_file


# to copy files to GCP bucket
# from copy_to_bucket_via_key import setup_logging, authenticate, copy_files_to_gcp
from copy_to_bucket_via_wif import setup_logging, copy_files_to_gcp
from zip_utilities import append_zip_from_files



class BusinessTaskError(Exception):
    """Custom exception for business task errors with execution context."""
    def __init__(self, message, execution_context=None, original_exception=None):
        context = f"Context: {execution_context}" if execution_context else ""
        full_message = f"{message}. {context}"
        if original_exception:
            full_message += f" | Original error: {original_exception}"
        super().__init__(full_message)
        self.execution_context = execution_context
        self.original_exception = original_exception



# class QueryType(Enum):
#     SQL = "SQL"
#     SP = "SP"


def is_none_or_empty(s):
    return s is None or s == ""


def parse_task_splitindex_format(task_splitindex_format: str) -> Tuple[str, float, str, bool]:
    """
    Parses the given string pattern to split into index format, file size, size units, 
    and zip option.

    Parameters:
    - task_splitindex_format: A string pattern in the 
    format `{index}|size|zip_option`, e.g., `{_index}|500MB|NOZIP`.

    Returns:
    - A tuple with four elements: (index_format, max_file_size, size_units, zip_nozip).

    # Example usage
    # try:
    #     result = parse_task_splitindex_format("{_index}|500MB|NOZIP")
    #     print(result)  # Output: ('{_index}', 500.0, 'MB', False)

    #     result = parse_task_splitindex_format("_part{index}|300.5KB|ZIPFILES")
    #     print(result)  # Output: ('_part{index}', 300.5, 'KB', True)
    # except ValueError as e:
    #     print(e)
    """
    # Split the string using the pipe character
    parts = task_splitindex_format.split('|')

    if len(parts) != 3:
        raise ValueError(
            "Input string does not match the expected pattern '{index}|size|zip_option'.")

    # Extract components from the split parts
    index_format: str = parts[0]   # The index format
    # The file size string, e.g., '500.5MB'
    max_file_size_str: str = parts[1].upper()
    # The zip option, either 'ZIPFILES' or 'NOZIP'
    zip_option_str: str = parts[2].upper()

    # Validate zip option and convert to boolean
    zip_nozip: bool
    if zip_option_str == "ZIPFILES":
        zip_nozip = True
    elif zip_option_str == "NOZIP":
        zip_nozip = False
    else:
        raise ValueError("Zip Files component must be 'ZIPFILES' or 'NOZIP'.")

    # Use regex to separate numeric file size and units (allow decimals)
    size_pattern = r'(\d+(?:\.\d+)?)(KB|MB|GB)'
    match = re.match(size_pattern, max_file_size_str)

    if not match:
        raise ValueError(
            "File size component does not match the expected format like '500MB' or '300.5KB'.")

    # Convert the numeric value to a float
    max_file_size: float = float(match.group(1))
    size_units: str = match.group(2)  # Extract the size units

    return index_format, max_file_size, size_units, zip_nozip


def prep_and_copy_to_bucket(
    file_list: List[str],
    task_grp_nm: str,
    destination_bucket: str,
    destination_path: str,
    wwconfig_path: str,
    gcp_project_name : str
) -> List[str]:
    """
        file_list          list: A list of filepaths to be copied.
        task_grp_nm         str: TASK_GRP_NM from configuration table [dbo].[ACONFIG_PROCESNG_RULES]
        source_path         str: Local path or network share.
        file_pattern        str: File name ("NH_HB_AdmissionSource_20250305.txt") or a pattern 
                                 of files to be copied ("NH_HB_*.txt")
        destination_bucket  str: A valid GCP bucket name
        destination_path    str: A valid folder path inside the GCP bucket
    """

    # Validate and prepare parameters to copy file to GCP bucket
    bucket_name: str
    if is_none_or_empty(destination_bucket):
        # Raise an error if the destination bucket is not provided
        raise ValueError("Destination bucket is required.")
    else:
        bucket_name = destination_bucket

    # Prepare the folder name to be used inside the GCP bucket
    #
    # This is a temporary fix to only keep the last part of the path as the destination
    # folder in GCP. Soon all the processing will be done on the GCP environment and
    # copying files to GCP will be unnecessary
    UNWANTED_PART = r'\\nasdata204\sharedata\HealthcareAnalyticSecureTransfers-PHI\Epic_Extracts' \
        + '\\'

    final_destination_folder = destination_path.replace(
        UNWANTED_PART, "").replace('\\', '/')

    # DEBUG CODE
    print('destination_path = ', destination_path)
    print('UNWANTED_PART = ', UNWANTED_PART)
    print("final_destination_folder = ", final_destination_folder)

    copied_files: List[str] = []

    if not isinstance(file_list, list) or not file_list:
        return copied_files  # Return empty list if not a valid list or is empty

    # set up credentials
    # key_path: str = r"\\SWBDEPICBISQL1v\GCPkey\rgclnp-p-l-cdm-processing1-3fc68f785cf9.json"
    # credentials = authenticate(key_path)

    # Traverse list of files to prepare their necessary parameters to copy them to a GCP bucket
    for file_path in file_list:
        try:

            # Separate the components of the source path
            folder_path = os.path.dirname(file_path)
            file_name_with_ext = os.path.basename(file_path)
            file_name, file_ext = os.path.splitext(file_name_with_ext)

            # set up logging
            log_file_name = task_grp_nm + "_gcp_copy.log"
            log_full_path = os.path.join(folder_path, log_file_name)
            setup_logging(log_file=log_full_path, console_output=True)

            # Copy file(s) to GCP bucket
            copy_files_to_gcp(source_path=folder_path,
                              pattern=file_name_with_ext,
                              bucket_name=bucket_name,
                              destination_folder=final_destination_folder,
                              wwconfig_path=wwconfig_path,
                              gcp_project_name=gcp_project_name
                              )

            # Add the returned destination URI to the list of copied files.
            copied_files.append(file_path)

        except Exception as e:  # Catch and re-raise any exceptions from prep_and_copy_to_bucket
            print(f"Error copying file {file_path} to GCP: {e}")
            raise

    return copied_files


def convert_size_to_bytes(size, unit):
    """Converts file size with unit to bytes."""
    unit = unit.upper()
    if unit == "KB":
        return size * 1024
    elif unit == "MB":
        return size * 1024 * 1024
    elif unit == "GB":
        return size * 1024 * 1024 * 1024
    else:
        raise ValueError("Invalid size unit. Use KB, MB, or GB.")


def rename_tmp_file(temp_filename: str, filename: str):
    # check if the target filename already exists in the folder, if so
    #  delete it to replace it with the new file
    if os.path.exists(filename):
        try:
            print(f"Deleting existing file '{filename}'.")
            os.remove(filename)
            print(f"Existing file '{filename}' deleted successfully.")
        except OSError as e:
            print(f"Error deleting file '{filename}': {e}")
            # Handle the error appropriately, e.g., exit the script or skip the rename
            raise
    else:
        print(f"File '{filename}' does not exist.")

    # Rename the temp file to the proper file name
    try:
        print(f"Renaming file '{temp_filename}' to '{filename}'.")
        os.rename(temp_filename, filename)
        print(f"File '{temp_filename}' renamed to '{filename}' successfully.")
    except OSError as e:
        print(f"Error renaming file: {e}")
        # Handle the error as needed
        raise



def write_data_to_multiple_files(
    cursor,
    filename: str,
    delimiter: str = ',',
    include_headers: bool = True,
    quoting=csv.QUOTE_MINIMAL,
    force_single_file=True,
    batch_size: int = 1000,
    max_file_size: float = 1.95,
    size_unit: str = 'GB',
    index_format: str = "_{index:01d}",
    encoding: str = 'utf-8'
):
    """
    Writes data from an SQL Server cursor to a single or multiple delimited
    text files based on the force_single_file parameter. Uses a .tmp extension
    for the single file while writing and renames it at the end.

    Parameters:
        cursor (pyodbc.Cursor): A pre-executed pyodbc cursor object ready to fetch data.
        filename (str): The base filename for the output file(s).  The extension will be extracted.
        max_file_size (float): The maximum size of each output file in multi-file mode (ignored 
                               if force_single_file is True).
        size_unit (str): The unit for max_file_size (KB, MB, GB).  
                         Ignored if force_single_file is True.
        index_format (str, optional): The format string for the file index in multi-file mode. 
                                      Defaults to "_{index:01d}". Ignored if force_single_file is True.
        delimiter (str, optional): The delimiter character used in the output file(s). 
                                   Defaults to ','.
        include_headers (bool, optional): Whether to include column headers in the output file(s). 
                                          Defaults to True.
        quoting (int, optional): Quoting option for csv.writer 
                                 (e.g., csv.QUOTE_MINIMAL, csv.QUOTE_ALL). 
                                 Defaults to csv.QUOTE_MINIMAL.
        batch_size (int, optional): Number of database rows to fetch at a time. Defaults to 1000.
        force_single_file (bool, optional): If True, writes all data to a single file. If False, 
                                            uses multi-file mode. Defaults to True.

    Returns:
        tuple: A tuple containing 
               (total_rows_read, total_rows_written, errors_encountered, generated_files).
               generated_files is a list of the filenames created.

    Raises:
        ValueError: If the total rows read do not match the total rows written.
    """

    operation_context: str = ''

    total_rows_read: int = 0
    total_rows_written: int = 0
    errors_encountered: int = 0
    generated_files: List[str] = []
    column_names: List[str] = []

    # utility function to write a batch of date to a text file
    def write_batch_to_file(writer, rows):
        """Helper function to write rows and update total_rows_written *once*."""
        nonlocal total_rows_written
        writer.writerows(rows)  # Write all rows at once
        total_rows_written += len(rows)  # Increment only once per batch


    temp_filename: str = ''
    final_filename: str = ''

    if force_single_file:  # Single file mode
        base_filename, base_ext = os.path.splitext(filename)
        temp_filename = f"{base_filename}.tmp"

        try:

            operation_context = f"Creating temporary file in write mode '{temp_filename}'"

            with open(temp_filename, mode='w', newline='', encoding=encoding, errors='replace') as file:


                writer = csv.writer(file, delimiter=delimiter,
                                    quotechar='"', quoting=quoting)
                if include_headers:
                    column_names = [column[0] for column in cursor.description]

                    operation_context = "Writing file header to temporary file"
                    writer.writerow(column_names)

                while True:
                    operation_context = "Fetching data rows from cursor"
                    rows = cursor.fetchmany(batch_size)
                    total_rows_read += len(rows)
                    if not rows:
                        break
                    operation_context = "Writing data row to temporary file"
                    write_batch_to_file(writer, rows)

        except IOError as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "IO error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except (pyodbc.Error) as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Database error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except Exception as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Unexpected error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex

        try:
            operation_context = f"Renaming temp file '{temp_filename}' to the expected file name '{filename}'"
            rename_tmp_file(temp_filename, filename)

            operation_context = "Adding newly-generated file name to the list"
            generated_files.append(filename)

        except IOError as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "IO error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except Exception as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Unexpected error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex

    else:  # Multi-file mode

        max_bytes = convert_size_to_bytes(max_file_size, size_unit)
        file_index = 1
        base_filename, base_ext = os.path.splitext(filename)

        current_file_size: float = 0.0
        remaining_rows = []  # Store rows that didn't fit in the previous file

        if include_headers:
            column_names = [column[0] for column in cursor.description]

        try:

            operation_context = "Reading the first batch of data from the cursor"
            rows = cursor.fetchmany(batch_size)
            total_rows_read += len(rows)

            # if there are no rows in the cursor, no files need to be created
            if total_rows_read == 0:
                print("No rows were returned from the cursor. No files will be created.")
            else:

                # begin with all the rows read in the initial batch
                remaining_rows = rows
                header_size: float
                accumulated_size: float

                while True:

                    # build the temporary and final file names for the file to be generated
                    formatted_index = calc_seq_index(index_format, file_index)
                    temp_filename = f"{base_filename.replace('{SPLITINDEX}', formatted_index)}.tmp"
                    final_filename = f"{base_filename.replace('{SPLITINDEX}', formatted_index)}{base_ext}"

                    operation_context = f"Creating a new output file '{temp_filename}'"
                    with open(temp_filename, mode='w', newline='', encoding=encoding, errors='replace') as file:
                        writer = csv.writer(
                            file, delimiter=delimiter, quotechar='"', quoting=quoting)

                        current_file_size = 0.0

                        if include_headers:
                            operation_context = f"Writing headers to output file '{temp_filename}'"
                            writer.writerow(column_names)
                            header_size = sum(len(str(
                                name)) for name in column_names) + len(delimiter) * (len(column_names) - 1) + 2
                            current_file_size += header_size

                        while True:  # Process remaining_rows and new batches

                            if remaining_rows:  # Write any leftover rows from previous file first
                                string_buffer = io.StringIO()
                                temp_writer = csv.writer(
                                    string_buffer, delimiter=delimiter, quotechar='"', quoting=quoting)
                                rows_to_write = []
                                accumulated_size = 0.0
                                for row in remaining_rows:
                                    row_string = io.StringIO()
                                    temp_row_writer = csv.writer(
                                        row_string, delimiter=delimiter, quotechar='"', quoting=quoting)
                                    temp_row_writer.writerow(row)
                                    row_str = row_string.getvalue()
                                    row_size = len(row_str.encode('utf-8'))
                                    if accumulated_size + row_size < max_bytes - current_file_size:
                                        accumulated_size += row_size
                                        rows_to_write.append(row)
                                    else:
                                        break    # Innermost loop to write the rows that fit

                                operation_context = "Only incrementing total_rows_written here, when writing to the actual file"
                                write_batch_to_file(writer, rows_to_write)
                                current_file_size += accumulated_size
                                
                                # Remove only the rows that were written
                                remaining_rows = remaining_rows[len(rows_to_write):]
                                if not remaining_rows:
                                    remaining_rows = []  # Reset remaining_rows after processing

                            if not remaining_rows:  # Only fetch if remaining_rows is empty
                                rows = cursor.fetchmany(batch_size)
                                total_rows_read += len(rows)

                                if not rows:  # No more rows from cursor
                                    if current_file_size > 0:
                                        file.flush()  # Ensure all buffered data is written before closing
                                        file.close()  # Explicitly close the file before renaming

                                        operation_context = f"Renaming temp file '{temp_filename}' to the expected file name '{final_filename}'"
                                        rename_tmp_file(
                                            temp_filename, final_filename)

                                        # add newly-generated file name to the list
                                        generated_files.append(final_filename)

                                    break  # Exit inner loop (no more rows)

                                string_buffer = io.StringIO()
                                temp_writer = csv.writer(
                                    string_buffer, delimiter=delimiter, quotechar='"', quoting=quoting)
                                temp_writer.writerows(rows)
                                batch_string = string_buffer.getvalue()
                                batch_size_bytes = len(
                                    batch_string.encode('utf-8'))

                                if current_file_size + batch_size_bytes < max_bytes:
                                    current_file_size += batch_size_bytes
                                    file.write(batch_string)
                                    # Only increment total_rows_written here
                                    total_rows_written += len(rows)
                                else:  # Remaining rows handling # Time to create a new file
                                    remaining_rows = rows  # Store the entire batch for next file

                                    file.flush()  # Ensure all buffered data is written before closing
                                    file.close()  # Explicitly close the file before renaming

                                    # rename temp file to the expected file name
                                    operation_context = f"Renaming temp file '{temp_filename}' to the expected file name '{final_filename}'"
                                    rename_tmp_file(
                                        temp_filename, final_filename)

                                    # add newly-generated file name to the list
                                    generated_files.append(final_filename)

                                    break  # Exit inner loop to create a new file due to size limit

                            elif remaining_rows:  # if there are still remaining rows from the previous file

                                file.flush()  # Ensure all buffered data is written before closing
                                file.close()  # Explicitly close the file before renaming

                                # rename temp file to the expected file name
                                operation_context = f"Renaming temp file '{temp_filename}' to the expected file name '{final_filename}'"
                                rename_tmp_file(temp_filename, final_filename)

                                # add newly-generated file name to the list
                                generated_files.append(final_filename)

                                break

                    if not rows:  # No more rows to fetch
                        break  # Exit the outer loop when there are no more rows to fetch
                    file_index += 1

        except IOError as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "IO error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except (pyodbc.Error) as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Database error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except Exception as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Unexpected error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex

    if total_rows_read != total_rows_written:
        raise ValueError(
            f"Row count mismatch: read {total_rows_read}, but wrote {total_rows_written}.")

    return total_rows_read, total_rows_written, errors_encountered, generated_files


def copy_files_to_destination(generated_files, destination_folder, create_destination=False):
    """
    Copies a list of files to a destination folder.

    Parameters:
        generated_files (list): A list of filepaths to be copied.
        destination_folder (str): The destination folder path.
        create_destination (bool, optional): If True, creates the destination 
                                            folder if it doesn't exist. 
                                            If False, raises an error if the 
                                            destination folder doesn't exist. 
                                            Defaults to False.


    Returns:
        list: A list of the filepaths of the copied files in the 
              destination folder. Returns an empty list if the 
              generated_files parameter is an empty list, or if it
              is not a list.

    Raises:
        FileNotFoundError: If a source file does not exist.
        IOError: If there are other issues during copying.
        ValueError: If the destination folder does not exist and 
                    create_destination is False.

    """
    copied_files = []

    if not isinstance(generated_files, list) or not generated_files:
        # Return empty list if not a valid list or is empty.
        return copied_files

    if not create_destination:  # Raise error if destination folder does not exist
        if not os.path.exists(destination_folder):
            raise ValueError(
                f"Destination folder '{destination_folder}' does not exist.")
    else:
        os.makedirs(destination_folder, exist_ok=True)

    for file_path in generated_files:
        try:
            destination_path = os.path.join(
                destination_folder, os.path.basename(file_path))
            shutil.copy2(file_path, destination_path)
            copied_files.append(destination_path)

        except FileNotFoundError:
            print(f"Error: Source file not found: {file_path}")
            raise  # Re-raise the exception
        except IOError as e:
            print(f"Error copying file {file_path}: {e}")
            raise  # Re-raise to stop if copy fails

    return copied_files


def str2bool(v) -> bool:
    """Convert a string to a boolean value."""  # Improved validation
    if isinstance(v, bool): # Already a boolean (e.g. default)
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')
    

def validate_and_start_extract() -> None:
    """
    Produce a data extract for a given Task Name
    """
    operation_context: str = ''
    
    try:

        # Define the command-line arguments expected by the script
        parser = argparse.ArgumentParser(description="Process script parameters.")
        parser.add_argument("bi_server", type=str, help="BI SQL Server name")
        parser.add_argument("bi_database", type=str, help="BI Database name")
        parser.add_argument("task_name", type=str, help="Task name")
        parser.add_argument("task_process_step_id", type=int, help="Task process step ID")
        parser.add_argument("copy_files_to_archive", nargs='?',type=str2bool, default=False, help="Copy files to archive")
        parser.add_argument("copy_files_to_gcp_bucket", nargs='?',type=str2bool, default=False, help="Copy files to GCP bucket")
        parser.add_argument("wwconfig_path", nargs='?',type=str, default="", help="Path to the Workload Identity Federation config file")

        args = parser.parse_args()

        # Accessing mandatory positional command-line arguments
        ConnEpicBIServer = args.bi_server
        ConnEpicBIDatabase = args.bi_database
        TaskName = args.task_name
        TaskProcessStepID = args.task_process_step_id
        wwconfig_path: str = ""


        # Print mandatory positional command-line arguments
        print(f"BI Server: {ConnEpicBIServer}")
        print(f"BI Database: {ConnEpicBIDatabase}")
        print(f"Task Name: {TaskName}")
        print(f"Task Process Step ID: {TaskProcessStepID}")

        # Accessing optional command-line arguments
        # Checking for presence of optional positional arguments:
        if args.copy_files_to_archive:
            copy_files_to_archive = args.copy_files_to_archive
        else:
            copy_files_to_archive = False

        if args.copy_files_to_gcp_bucket:
            copy_files_to_gcp_bucket = args.copy_files_to_gcp_bucket

            # If copying files to GCP bucket is enabled, then validate the WIF config path
            wwconfig_path = args.wwconfig_path
            if is_none_or_empty(wwconfig_path):
                # Raise an error if the WIF config path is not provided
                raise ValueError("Path to the Workload Identity Federation config file is required.")
        else:
            copy_files_to_gcp_bucket = False


        # Print optional positional command-line arguments
        print(f"Copy files to archive: {copy_files_to_archive}")
        print(f"Copy files to GCP bucket: {copy_files_to_gcp_bucket}")
        print(f"WIF config path: {wwconfig_path}")


        # Create a connection string for BI SQL Server
        ConnectionStringEpicBI = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + \
            ConnEpicBIServer + ';DATABASE='+ ConnEpicBIDatabase + \
            ';Trusted_Connection=yes;TrustServerCertificate=yes'
        print(f"Connection string to BI Server: {ConnectionStringEpicBI}")
        ConnEpicBI = pyodbc.connect(ConnectionStringEpicBI)
        ConnEpicBI.autocommit = True
        cursorEpicBI = ConnEpicBI.cursor()
        cursorEpicBIlog = ConnEpicBI.cursor()

        # Initialize counters and names
        source_record_count: int = 0
        target_record_count: int = 0
        file_part_name = ''
        date_part_name = ''
        TargetName = None


        get_task_info_sql = (
            "SELECT TASK_ID,TASK_NM,EXTRCT_FILE_NM_FORMAT,EXTRCT_FILE_DELIMETR_TYPE"
            ",TASK_INCRMNTL_FLAG,TASK_DESTINATION_TYPE,EXTRCT_DESTINATION_PATH"
            ",EXTRCT_ARCHIVE_PATH,TASK_ENV_TYPE_NAME,TASK_DB_NM,TASK_SERVER_NM"
            ",TASK_PRIORITY_NBR,TASK_CODE_TYPE,TASK_CODE,TASK_GRP_ID"
            ",DESTINATION_SERVER_NM,DESTINATION_DB_NM,DESTINATION_TBL_NM"
            ",DESTINATION_TBL_TRUNC_FLAG,TASK_PARAMS,FORMAT_FILE_TYPES"
            ",TASK_SPLITINDEX_FORMAT, TASK_GRP_NM,EXTRCT_DESTINATION_GCP_BUCKET"
            ",EXTRCT_DESTINATION_GCP_PROJECT,EXTRCT_COPY_DESTINATION_PATH"
            ",EXTRCT_ZIP_TYPE,EXTRCT_ZIP_FILE_NM_FORMAT,EXTRCT_FILE_ENCODING_NAME "
            " FROM [dbo].[ACONFIG_PROCESNG_RULES]"
            " WHERE 1=1 "
            " AND TASK_ACTV_FLG =1"
            " AND TASK_NM ='" + TaskName + "'"
        )

        print("get_task_info_sql:\n", get_task_info_sql)

        # Read Task related Data from the Table
        # cursorEpicBI.execute("SELECT TASK_ID,TASK_NM,EXTRCT_FILE_NM_FORMAT,EXTRCT_FILE_DELIMETR_TYPE,TASK_INCRMNTL_FLAG,TASK_DESTINATION_TYPE,EXTRCT_DESTINATION_PATH,EXTRCT_ARCHIVE_PATH"
        #                      + ",TASK_ENV_TYPE_NAME,	TASK_DB_NM,	TASK_SERVER_NM,	TASK_PRIORITY_NBR,	TASK_CODE_TYPE,	TASK_CODE,TASK_GRP_ID"
        #                      + ",DESTINATION_SERVER_NM ,DESTINATION_DB_NM ,DESTINATION_TBL_NM,DESTINATION_TBL_TRUNC_FLAG,TASK_PARAMS,FORMAT_FILE_TYPES "
        #                      + ",TASK_SPLITINDEX_FORMAT, TASK_GRP_NM, EXTRCT_DESTINATION_GCP_BUCKET, EXTRCT_DESTINATION_GCP_PROJECT "
        #                      + ",EXTRCT_COPY_DESTINATION_PATH, EXTRCT_ZIP_TYPE, EXTRCT_ZIP_FILE_NM_FORMAT, EXTRCT_FILE_ENCODING_NAME "
        #                      + " FROM [dbo].[ACONFIG_PROCESNG_RULES]"
        #                      + " WHERE 1=1 "
        #                      + " AND TASK_ACTV_FLG =1"
        #                      + " AND TASK_NM ='"+TaskName + "'")

        # fetch task data
        cursorEpicBI.execute(get_task_info_sql)

        # check if task data exists, if not display message and raise an error to abort execution
        if cursorEpicBI.rowcount == 0:
            print(f"Error:No ACTIVE task found in the configuration table with TASK name: {TaskName}")
            raise ValueError(f"Error: No ACTIVE task found in the configuration table with TASK name: {TaskName}")
        
        # if cursorEpicBI.rowcount == 0:
        #     print(f"No active task found in the configuration table with name: {TaskName}")
        #     return

        TaskData = cursorEpicBI.fetchone()
        Extract_File_NM_Format = TaskData[2]
        Extract_File_Delimeter = TaskData[3]
        Task_Incrmntl_Flag = TaskData[4]
        Task_Dest_Type = TaskData[5]
        File_Dest_Path = TaskData[6]
        File_Archive_Path = TaskData[7]
        Task_Env_Type_NM = TaskData[8]
        Task_DB_NM = TaskData[9]
        Task_Server_NM = TaskData[10]
        Task_Code_Type = TaskData[12]
        Task_CodeQuery = TaskData[13]
        Task_Grp_Id = TaskData[14]
        Destn_Server_Nm = TaskData[15]
        Destn_DB_Nm = TaskData[16]
        Destn_TBL_NM = TaskData[17]
        Dest_TBL_Trunct_Flag = TaskData[18]
        Task_param = TaskData[19]
        FORMAT_FILE_TYPES = TaskData[20]       # FORMAT_FILE_TYPES
        task_splitindex_format = TaskData[21]  # TASK_SPLITINDEX_FORMAT
        task_grp_nm = TaskData[22]             # TASK_GRP_NM
        extrct_destination_gcp_bucket = TaskData[23]  # EXTRCT_DESTINATION_GCP_BUCKET
        extrct_destination_gcp_project = TaskData[24]  # EXTRCT_DESTINATION_GCP_PROJECT
        extrct_copy_destination_path = TaskData[25]  # EXTRCT_COPY_DESTINATION_PATH
        #extrct_copy_archive_path = TaskData[26]  # EXTRCT_COPY_ARCHIVE_PATH
        extrct_zip_type = TaskData[26]  # EXTRCT_ZIP_TYPE
        extrct_zip_file_nm_format = TaskData[27]  # EXTRCT_ZIP_FILE_NM_FORMAT
        extrct_file_encoding_name = TaskData[28]  # EXTRCT_FILE_ENCODING_NAME

        Include_Header = bool(1)

        # Format_File_Type ='NO_HEADER|ADD_QUOTE'
        # Determine whether to include header
        Include_Header = bool(0) if (not is_none_or_empty(FORMAT_FILE_TYPES)) and ("NO_HEADER" in FORMAT_FILE_TYPES.upper()) else bool(1)

        # ADD_Quote = 1 if "ADD_QUOTE" in File_Format_Desc.upper() else 0
        # Determine whether to quote fields
        ADD_Quote = csv.QUOTE_ALL if (not is_none_or_empty(FORMAT_FILE_TYPES)) and ("ADD_QUOTE" in FORMAT_FILE_TYPES.upper()) else csv.QUOTE_MINIMAL


        # From the Audit Tables get the incrmental Date from the last sucessful process
        sql_get_last_incremental_date = (
            "SELECT ISNULL(LAST_INCRMNTL_DT,'1900-01-01') AS LAST_INCRMNTL_DT "
            "FROM AAUDIT_TBL_LOG (NOLOCK) "
            "WHERE PRCS_ID = ( SELECT MAX(PRCS_ID) FROM APROCESS_TBL_LOG (NOLOCK) "
            "WHERE 1=1 AND PRCS_STAT ='SUCCESSFUL' "
            "AND TASK_GRP_ID ='" + Task_Grp_Id + "') "
            "AND PRCS_TASK_NM ='" + TaskName + "'"
        )

        print("sql_get_last_incremental_date:\n", sql_get_last_incremental_date)

        # Execute the query to fetch the Last Incremental Date from the Audit table
        cursorEpicBI.execute(sql_get_last_incremental_date)

        # Get the first row from the query results, which should contain the Last Incremental Date from the Audit table
        last_incremental_row = cursorEpicBI.fetchone()

        # Store the number of rows returned in last_incremental_row
        last_incremental_row_count = cursorEpicBI.rowcount


        # Validate if the there is a value else default to three days from the current date
        if last_incremental_row_count < 1 or last_incremental_row is None:
            #or last_incremental_row == (datetime(1900, 1, 1).date(),):

            # Get the current date and time
            current_date = datetime.now().date()

            # Subtract three days from the current date
            last_incremental_date = (current_date - timedelta(days=3)).strftime("%Y-%m-%d")

        else:
            last_incremental_date = last_incremental_row[0]

        print("last_incremental_date: ",last_incremental_date)

        # modify the code based on if its a SQL Query or a Stored Procedure
        if Task_Code_Type == 'SQL':
            Task_CodeQuery = Task_CodeQuery

        elif Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 0:
            Task_CodeQuery = "EXEC " + Task_CodeQuery

        elif Task_Code_Type == 'SP' and Task_Incrmntl_Flag == 1:
            Task_CodeQuery = (
                "DECLARE @OutputDate DATE; "
                "EXEC " + Task_CodeQuery + " "
            )

        # elif Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
        #     Task_CodeQuery = " DECLARE @output_param DATE; EXEC " + Task_CodeQuery + \
        #         " @INputDate = ?, @outputDate = @output_param OUTPUT; SELECT @output_param AS out_param_value;"

        # elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
        #     Task_CodeQuery = (
        #         "DECLARE @OutputDate DATE; "
        #         "EXEC " + Task_CodeQuery + \
        #         " @InputDate = ?, @OutputDate = @OutputDate OUTPUT;"
        #         " SELECT @OutputDate AS out_param_value;"
        #     )

            # get parameter information for the stored procedure
            # get_stored_procedure_parameters(cursor, procedure_name: str)

        elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param):
            Task_CodeQuery = Task_CodeQuery


        print("Task_CodeQuery: ",Task_CodeQuery)

        # Connect to Task code SQL Server using Windows auth
        # ConnectStringTaskCode = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + \
        #     Task_Server_NM+';DATABASE='+Task_DB_NM+';Trusted_Connection=yes'
        ConnectStringTaskCode = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + \
            Task_Server_NM+';DATABASE='+Task_DB_NM + \
            ';Trusted_Connection=yes;TrustServerCertificate=yes'
        ConnTaskCode = pyodbc.connect(ConnectStringTaskCode)
        ConnTaskCode.autocommit = True
        cursorTaskCode = ConnTaskCode.cursor()

        print("Task_param: ", Task_param)

        if not is_none_or_empty(Task_param):

            try:
                operation_context = "Executing query to fetch Task Parameters"
                cursorTaskCode.execute(Task_param)
            except Exception as ex:
                context = {
                    "operation": operation_context
                }
                raise BusinessTaskError(
                    "Unexpected error occurred while executing query to fetch Task Parameters",
                    execution_context=context,
                    original_exception=ex
                ) from ex

            try:
                operation_context = "Fetching all rows from query to fetch Task Parameters"
                Task_param_rows = cursorTaskCode.fetchall()
            except Exception as ex:
                context = {
                    "operation": operation_context
                }
                raise BusinessTaskError(
                    "Unexpected error occurred while fetching all rows from query to fetch Task Parameters",
                    execution_context=context,
                    original_exception=ex
                ) from ex
            
            TaskParam_names = []  # Initialize an empty list to store parameter names

            print("Task_param_rows length: ", len(Task_param_rows))
            if len(Task_param_rows) == 0:
                print("\tWARNING: No rows found for Task_param.")
                print("\tTask_param: ", Task_param)
                print("\tInvestigate why no rows were found for Task_param.")


            # # Retrieve column description
            # column_descriptions = cursorTaskCode.description

            # # Process column metadata
            # for column in column_descriptions:
            #     print(f"Column Name: {column[0]}")
            #     print(f"Type: {column[1]}")
            #     print(f"Display Size: {column[2]}")
            #     print(f"Nullable: {column[3]}")


            # Get Column Names for the Task parameter

            # Get column names
            TaskParamColumn_names = [column[0]
                                     for column in cursorTaskCode.description]
            print("TaskParamColumn_names: ", TaskParamColumn_names)
            # Find the filename and date columns
            filename_column = next(
                (col for col in TaskParamColumn_names if 'file_name_part' in col.lower()), None)
            date_column = next(
                (col for col in TaskParamColumn_names if 'date_part' in col.lower()), None)

            for i in cursorTaskCode.description:
                TaskParam_name = f"@{i[0]}"
                # Add the parameter name to the param_names list
                TaskParam_names.append(TaskParam_name)

            if filename_column and date_column:
                TaskParam_names = TaskParam_names[:-2]
            elif filename_column or date_column:
                TaskParam_names = TaskParam_names[:-1]
            else:
                TaskParam_names = TaskParam_names[:]

            
            # Construct the SQL parameter placeholders
            Taskparam_placeholders = ', '.join(
                f"{ParamName}=?" for ParamName in TaskParam_names)

            # Construct the SQL query with parameter placeholders
            if not (Task_Code_Type == 'SP' and Task_Incrmntl_Flag == 1):
                # Append parameter placeholders to Task_CodeQuery for non-incremental SQL code
                Task_CodeQuery = f"{Task_CodeQuery} {Taskparam_placeholders}"
                
                print("Task_CodeQuery: ", Task_CodeQuery)
            else:
                if Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
                    # Append OUTPUT parameter if incremental flag is set and it is a stored procedure
                    # and if Task_param is empty
                    Task_CodeQuery = (
                        Task_CodeQuery + " @OutputDate = @OutputDate OUTPUT;"
                    )
                elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
                    # Append OUTPUT parameter if incremental flag is set and it is a stored procedure
                    # and if Task_param is not empty
                    Task_CodeQuery = (
                        Task_CodeQuery + " " + Taskparam_placeholders + 
                        ", @OutputDate = @OutputDate OUTPUT;"
                )
            print("Task_CodeQuery with parameter placeholders: ", Task_CodeQuery)

        else:
            Task_param_rows = [(1, "NAME")]

        for TaskParamRecords in Task_param_rows:

            task_params_values = []  # Initialize an empty list to store parameter values

            if not is_none_or_empty(Task_param):
                if filename_column and date_column:

                    # Excluding the file name and date column
                    Taskparams = TaskParamRecords[:-2]
                    # Get the file name from the filename column
                    file_part_name = TaskParamRecords[-2]
                    date_part_name = TaskParamRecords[-1]
                    if len(TaskParamRecords) > 2:
                        # Excluding the file name or date column
                        Taskparams = TaskParamRecords[:-2]
                        TaskParamRecords = TaskParamRecords[:-2]
                    else:
                        Taskparams = []
                        TaskParamRecords = []

                elif filename_column or date_column:
                    if filename_column:
                        file_part_name = TaskParamRecords[-1]
                    if date_column:
                        date_part_name = TaskParamRecords[-1]
                    if len(TaskParamRecords) > 1:
                        # Excluding the file name or date column
                        Taskparams = TaskParamRecords[:-1]
                        TaskParamRecords = TaskParamRecords[:-1]
                    else:
                        Taskparams = []
                        TaskParamRecords = []

                else:
                    Taskparams = TaskParamRecords[:]

            # Taskparams = TaskParamRecords[:-1]  # Excluding the file name column

            # Iterate over the columns in the row, excluding the last column (filename)
            # for i, value in enumerate(TaskParamRecords[:-1], start=1):
            for i, value in enumerate(TaskParamRecords[:], start=1):
                # TaskParam_name = f"@Param{i}"  # Dynamically generate parameter name
                # Add the value to the params list
                task_params_values.append(value)

            print('task_params_values: ', task_params_values)

            # Execute Task Code query
            # Prepares and runs SQL commands but does not itself fetch data from a SELECT query
            if Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:

                # If Task_param is empty, use last_incremental_date
                params = (last_incremental_date,)

                print(f"params: {params}")
                print(f"Task_CodeQuery: {Task_CodeQuery}")

                # Inside a try/except block, execute the stored procedure
                try:
                    cursorTaskCode.execute(Task_CodeQuery, params)
                except Exception as ex:
                    context = {
                        "operation": "Executing stored procedure with last incremental date"
                    }
                    raise BusinessTaskError(
                        "Unexpected error occurred while executing stored procedure with last incremental date",
                        execution_context=context,
                        original_exception=ex
                    ) from ex

            elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
            
                # If task_params_values is empty, use last_incremental_date
                if not task_params_values:
                    params = (last_incremental_date,)
                else:
                    params = tuple(task_params_values)

                print(f"params: {params}")
                print(f"Task_CodeQuery: {Task_CodeQuery}")

                # Inside a try/except block, execute the stored procedure
                try:
                    cursorTaskCode.execute(Task_CodeQuery, params)
                except Exception as ex:
                    context = {
                        "operation": "Executing stored procedure with incremental date"
                    }
                    raise BusinessTaskError(
                        "Unexpected error occurred while executing stored procedure with incremental date",
                        execution_context=context,
                        original_exception=ex
                    ) from ex

            elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param):

                print(f"task_params_values: {task_params_values}")
                print(f"Task_CodeQuery: {Task_CodeQuery}")
                # print("pp" + params)

                # Execute the stored procedure with dynamic parameters
                # Inside a try/except block, execute the stored procedure
                try:
                    cursorTaskCode.execute(Task_CodeQuery, task_params_values)
                except Exception as ex:
                    context = {
                        "operation": "Executing stored procedure with dynamic parameters"
                    }
                    raise BusinessTaskError(
                        "Unexpected error occurred while executing stored procedure with dynamic parameters",
                        execution_context=context,
                        original_exception=ex
                    ) from ex
            else:
                # Inside a try/except block, execute the stored procedure
                try:
                    cursorTaskCode.execute(Task_CodeQuery)
                except Exception as ex:
                    context = {
                        "operation": "Executing SQL query/procedure without parameters"
                    }
                    raise BusinessTaskError(
                        "Unexpected error occurred while executing SQL query/procedure without parameters",
                        execution_context=context,
                        original_exception=ex
                    ) from ex


            # Check if the cursorTaskCode cursor is in a valid state
            if cursorTaskCode:
                # Cursor is open and ready
                # If the cursor did not produce any results, then raise an warning informing the caller that
                # the query returned no data and stop execution of the pipeline
                if cursorTaskCode.rowcount == 0:

                    # Update the audit log record for the current task
                    update_process_step_src_trgt = ("UPDATE dbo.AAUDIT_TBL_LOG "
                                                    "SET SRC_REC_CNT = ?,TRGT_REC_CNT = ? , TRGT_NM = ?, NOTES = ? "
                                                    "WHERE PRCS_STEP_ID = ?"
                    )
                    update_process_step_params = (
                        0, 0, TargetName, 'No data was found for this task', TaskProcessStepID)

                    print("update_process_step_src_trgt: ", update_process_step_src_trgt)
                    print("update_process_step_params: ", update_process_step_params)

                    cursorEpicBIlog.execute(update_process_step_src_trgt,
                                            update_process_step_params)
                    cursorEpicBIlog.commit()

                    # End the execution of this module gracefully
                    sys.exit(0)
                
            else:
                # Cursor is not available, add a context and raise an error
                context = {
                    "operation": "Checking cursor state after executing SQL query/procedure"
                }
                raise BusinessTaskError(
                    "Cursor is not available",
                    execution_context=context
                )

            # Evaluate if the Destination is a file and create file else skip to next step
            if Task_Dest_Type.upper() == "FILE":

                now = datetime.now()

                # Define different date formats
                # date_formats = ['%Y-%m-%d', '%d-%b-%y', '%m%d%y']

                # Find last underscore index
                last_idx = Extract_File_NM_Format.rindex('_')
                # Get substring after last underscore
                FileNMDateSubstr = Extract_File_NM_Format[last_idx+1:]
                # Get File Date Format
                FileDateFormat = FileNMDateSubstr.split(".")[0]

                if FileDateFormat.upper() == 'YYYYMMDD':
                    FileDateFormatString = '%Y%m%d'
                elif FileDateFormat.upper() == 'MMDDYYYY':
                    FileDateFormatString = '%m%d%Y'
                else:
                    FileDateFormatString = FileDateFormat

                # @EA START
                FileDateFormat_wo_SplitIndex = FileDateFormat.replace(
                    "{SPLITINDEX}", "")

                if FileDateFormat_wo_SplitIndex.upper() == 'YYYYMMDD':
                    FileDateFormatString = '%Y%m%d'
                elif FileDateFormat_wo_SplitIndex.upper() == 'MMDDYYYY':
                    FileDateFormatString = '%m%d%Y'
                else:
                    FileDateFormatString = FileDateFormat_wo_SplitIndex
                # @EA END

                # Validate date
                try:
                    # Get today's date
                    current_date = datetime.now()
                    # First, parse the input string into a datetime object
                    DateFormat = current_date.strftime(FileDateFormatString)

                    formatted_date = DateFormat
                except ValueError:
                    # Invalid date, use default
                    DateFormat = '%Y%m%d'
                    # Format the date
                    formatted_date = now.strftime(DateFormat)

                    # Construct file name with date format

                Extract_File_NM = Extract_File_NM_Format.replace(
                    FileDateFormat_wo_SplitIndex, formatted_date)

                # @EA
                Extract_File_NM = Extract_File_NM_Format.replace(
                    FileDateFormat_wo_SplitIndex, formatted_date)

                Extract_File_NM = Extract_File_NM.replace(
                    "<FILENAMEPART>", file_part_name)
                Extract_File_NM = Extract_File_NM.replace(
                    "<DATEPART>", date_part_name)

                # @EA
                # if not (task_splitindex_format == None):
                #    Extract_File_NM =Extract_File_NM.replace("{SPLITINDEX}", task_splitindex_format)

                TargetName = Extract_File_NM
                output_file = f'{File_Dest_Path}\\{Extract_File_NM}'
                archive_file_name: str = Extract_File_NM.replace(
                    "{SPLITINDEX}", "")
                Archive_Path = f'{File_Archive_Path}\\{archive_file_name}'

                print("output_file = ", output_file)
                print("Archive_Path = ", Archive_Path)

                # @EA
                # Read the data from the DB in small batches (N rows at a time) and write it to
                # the file to avoid memory overflow
                batch_size: int = 5000
                force_single_file: bool

                # Define the return variables after writing the data to the file(s)
                errors_encountered: int = 0
                generated_files: List[str] = []

                # Define variables used for multiple file generation and zipping
                file_seq_format: str
                max_file_size: float
                max_size_units: str
                # index_format: str
                zip_nozip: bool

                # Check if the extract requires to export the data to a single file or
                # to multiple files
                if is_none_or_empty(task_splitindex_format):
                    force_single_file = True
                    file_seq_format = ''
                    max_file_size = 50.0
                    max_size_units = 'GB'
                    zip_nozip = False
                else:
                    # The current extract requires export the data into multiple files
                    force_single_file = False

                    # Parse out the task_splitindex_format.
                    # Returns:
                    # A tuple with 4 elements: (index_format, max_file_size, size_units, zip_nozip).
                    # e.g. {index}|500MB should be split into: {index}, 500, MB

                    file_seq_format, max_file_size, max_size_units, zip_nozip = parse_task_splitindex_format(
                        task_splitindex_format)


                # Define the file encoding to be used
                extrct_file_encoding : str

                if not is_none_or_empty(extrct_file_encoding_name):
                    extrct_file_encoding = extrct_file_encoding_name
                else:
                    extrct_file_encoding = 'utf-8'

                # Export the data to one or multiple files
                try:
                    operation_context = f"Invoking method to export data to file '{output_file}'"
                    source_record_count, target_record_count, errors_encountered, generated_files = write_data_to_multiple_files(
                        cursor=cursorTaskCode,
                        filename=output_file,
                        delimiter=Extract_File_Delimeter,
                        include_headers=Include_Header,
                        quoting=ADD_Quote,
                        force_single_file=force_single_file,
                        batch_size=batch_size,
                        max_file_size=max_file_size,
                        size_unit=max_size_units,
                        index_format=file_seq_format,
                        encoding=extrct_file_encoding)
                except Exception as ex:
                    context = {
                        "operation": operation_context
                    }
                    raise BusinessTaskError(
                        "Unexpected error occurred while executing write_data_to_multiple_files method",
                        execution_context=context,
                        original_exception=ex
                    ) from ex
        
                
                print(f"Total Errors: {errors_encountered}")
                print("source_record_count = ", source_record_count)
                print("target_record_count = ", target_record_count)

                print(f"File exported to {output_file} successfully!")

                print(f"File part name: {file_part_name}")
                print(f"Date part name: {date_part_name}")

                print(f"Task Incremental Flag: {Task_Incrmntl_Flag}")

                print(f"Extract Zip Type: {extrct_zip_type}")
                print(f"Extract Zip File Name Format: {extrct_zip_file_nm_format}")

                # If necessary, perform zipping of the generated files
                if not is_none_or_empty(extrct_zip_type):
                    if extrct_zip_type.upper() == 'ZIP_GROUP':
                        # if the zip operation fails, log the error
                        try:

                            # build the full path of the log file based on the path of output_file
                            # and task_grp_nm
                            zipping_log_file_path = os.path.join(
                                os.path.dirname(output_file),
                                f"{task_grp_nm}_zipping.log"
                            )

                            # invoke zip utility function to append the files from the
                            # generated_files list to a zip file
                            zip_file_path = append_zip_from_files(
                                file_list=generated_files,
                                zip_file_name_format=extrct_zip_file_nm_format,
                                log_file_path=zipping_log_file_path
                            )
                            print("List of files successfully zipped to: ", zip_file_path)

                            # if the zip operation succeeded, then empty the generated_files list
                            if zip_file_path:
                                generated_files.clear()

                            # and add the zip file path to the generated_files list
                            generated_files.append(zip_file_path)

                        except (FileNotFoundError, IOError):
                            print("Error during file list zipping operation.")
                        except Exception as e:
                            print("Error occurred while zipping files.")
                            print(e)
                else:
                    print("No file zipping type provided. Therefore, no zipping operation performed.")



                # This change was implemented to make copies of specific MedMetrix extracts and
                # place them in one particular folder for vendors Sutherland, SMG,
                # and Revelation Care. However, it can be used for other extracts and vendors
                # as well.
                # If the variable extrct_copy_destination_path contains a valid path, copy the
                # generated file(s) to that location.
                if not is_none_or_empty(extrct_copy_destination_path):

                    # If the destination path exists, then proceed with the copy operation.
                    # If it does not exist, then log the error.
                    if os.path.exists(extrct_copy_destination_path):

                        # if the copy operation fails, log the error
                        try:
                            copy_files_to_destination(
                                generated_files=generated_files,
                                destination_folder=extrct_copy_destination_path,
                                create_destination=True
                            )
                            print("Extra copies of files successfully copied to: ", extrct_copy_destination_path)
                        except (FileNotFoundError, IOError):
                            print("Error during file copy operation. Check source files and destination folder.")
                else:
                    print("No extraction copy destination path provided. Therefore, no extra copy operation performed.")



                # Based on an extra command-line argument called copy_files_to_archive copy the generated file(s) to the Archive folder
                if copy_files_to_archive:
                    copied_file_list: List[str] = []
                    try:
                        copied_file_list = copy_files_to_destination(
                            generated_files=generated_files,
                            destination_folder=File_Archive_Path,
                            create_destination=False)
                        if copied_file_list:  # If not empty, everything was copied successfully
                            print("Files successfully copied to:", File_Archive_Path)
                            print("Copied files:", copied_file_list)
                        else:
                            print("No files copied to destination.")

                    except (FileNotFoundError, IOError):
                        print(
                            "Error during file copy operation.  Check source files and destination folder.")
                        raise
                
                if copy_files_to_gcp_bucket:
                    # Store a copy of the original generated file(s) to a GCP bucket
                    # Copy the file that was placed in the Archieve Path
                    copied_files_to_bucket: List[str] = []

                    # Prepare and copy files to GCP bucket
                    # If the copy operation fails, log the error
                    try:
                        copied_files_to_bucket = prep_and_copy_to_bucket(
                                file_list=generated_files,
                                task_grp_nm=task_grp_nm,
                                destination_bucket=extrct_destination_gcp_bucket,
                                destination_path=File_Dest_Path,
                                wwconfig_path=wwconfig_path,
                                gcp_project_name=extrct_destination_gcp_project
                        )

                        print('Copied files to GCP bucket:')
                        print(copied_files_to_bucket)

                    except Exception as e:
                        print("Error occurred while copying files to GCP bucket:", extrct_destination_gcp_bucket)
                        print(e)


            elif Task_Dest_Type.upper() == "TABLE":

                TargetName = Destn_TBL_NM
                # Connect to Destination SQL Server using Windows auth
                # ConnectStringSQLDestn = 'DRIVER={ODBC Driver 17 for SQL Server};SERVER=' + \
                #     Destn_Server_Nm+';DATABASE='+Destn_DB_Nm+';Trusted_Connection=yes'
                ConnectStringSQLDestn = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + \
                    Destn_Server_Nm+';DATABASE='+Destn_DB_Nm + \
                    ';Trusted_Connection=yes;TrustServerCertificate=yes'
                ConnSQLDest = pyodbc.connect(ConnectStringSQLDestn)
                ConnSQLDest.autocommit = True
                cursorSQLDest = ConnSQLDest.cursor()

                if Dest_TBL_Trunct_Flag == 1:
                    DestQuery = 'DELETE ' + Destn_TBL_NM
                    cursorSQLDest.execute(DestQuery)

                dest_query = "INSERT INTO " + Destn_TBL_NM + \
                    " VALUES (" + ", ".join(["?"] * len(TaskResults[0])) + ")"

                # Insert data into destination table
                for SourceRow in TaskResults:
                    cursorSQLDest.execute(dest_query, SourceRow)

                ConnSQLDest.commit()

                # Get the number of rows in the table after insertion
                Target_count_query = f"SELECT COUNT(*) FROM {Destn_TBL_NM}"
                cursorSQLDest.execute(Target_count_query)
                target_record_count = target_record_count + \
                    cursorSQLDest.fetchone()[0]
                cursorSQLDest.close()

            else:
                print("Skipping file creation step ")

            # Get output parameter
            if cursorTaskCode.nextset() and Task_Incrmntl_Flag == 1:
                Output_Param_row = cursorTaskCode.fetchall()
                for Output_param in Output_Param_row:
                    Output_date = Output_param[0]

                update_Process_Step_sql = "UPDATE dbo.AAUDIT_TBL_LOG SET LAST_INCRMNTL_DT = ? WHERE PRCS_STEP_ID = ?"

                update_params = (Output_date, TaskProcessStepID)
                cursorEpicBIlog.execute(update_Process_Step_sql, update_params)
                print(Output_date)
                print(update_params)
                cursorEpicBIlog.commit()

        print("\nUpdate the audit log record for the current task..")
        print("TargetName = ", TargetName)

        # Update the audit log record for the current task
        update_process_step_src_trgt = "UPDATE dbo.AAUDIT_TBL_LOG SET SRC_REC_CNT = ?,TRGT_REC_CNT = ? , TRGT_NM = ? WHERE PRCS_STEP_ID = ?"
        update_process_step_params = (
            source_record_count, target_record_count, TargetName, TaskProcessStepID)

        print("update_process_step_src_trgt: ", update_process_step_src_trgt)
        print("update_process_step_params: ", update_process_step_params)

        cursorEpicBIlog.execute(update_process_step_src_trgt,
                                update_process_step_params)

        print("file_part_name = ", file_part_name)
        print("date_part_name = ", date_part_name)

        cursorEpicBIlog.commit()

        ConnTaskCode.close()
        ConnEpicBI.close()

        print("Code Completed")
        sys.exit()

    except Exception as e:
        # Write error to stderr
        print(f"Error: {str(e)}", file=sys.stderr)
        # Exit with non-zero status code
        sys.exit(1)


def main() -> None:
    """Entry point for the script."""
    validate_and_start_extract()


if __name__ == '__main__':
    main()
