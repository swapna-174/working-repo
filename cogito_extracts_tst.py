"""
This script generates a text file for a given task defined in the configuration table
[dbo].[ACONFIG_PROCESNG_RULES] and places the file in a network share

Usage: 
    Can be called directly from the command line or integrated into other Python modules.

Main Features:
1) Generates one or more text files using data from Clarity or Caboodle
2) Places the file(s) in the "Outbound" folder of a network share
3) Optionally, places a 2nd copy of the file(s), taken from the "Outbound" folder,
   to the "Archive" folder of a network share
4) Optionally, places a 3rd copy of the file(s), taken from the "Outbound" folder, 
   to a specified folder in a Google Cloud Platform (GCP) bucket.

Args:
    conn_epic_bi_server  (str)     : Name of the BI SQL Server where the BI database
                                     resides
    conn_epic_bi_db      (str)     : Name of the BI database where the configuration
                                     table "[dbo].[ACONFIG_PROCESNG_RULES]" resides
    task_name            (str)     : Name of the task to process
    task_proc_step_id    (integer) : Numeric Task Process ID generated by the SSIS 
                                     package that invokes this script. Such ID is used
                                     for updating auditing records in the BI database.
    copy_files_to_archive (bool)   : If True, copies the generated file to the "Archive"
                                        folder in the network share. If False, does not.
    copy_files_to_gcp_bucket (bool): If True, copies the generated file to a GCP bucket. 
                                        If False, does not.
"""

import argparse
import sys
import os
import io
from datetime import datetime
from enum import Enum
import re
from typing import Tuple
from typing import List
import shutil
import csv
import pyodbc

# to split large text files into smaller ones
from cogito_utils_file_ops import calc_seq_index

# to copy files to GCP bucket
from copy_to_bucket_via_wif import setup_logging, copy_files_to_gcp
from zip_utilities import append_zip_from_files


class BusinessTaskError(Exception):
    """Custom exception for business task errors with execution context."""
    def __init__(self, message, execution_context=None, original_exception=None):
        context = f"Context: {execution_context}" if execution_context else ""
        full_message = f"{message}. {context}"
        if original_exception:
            full_message += f" | Original error: {original_exception}"
        super().__init__(full_message)
        self.execution_context = execution_context
        self.original_exception = original_exception


def is_none_or_empty(s):
    return s is None or s == ""


def parse_task_splitindex_format(task_splitindex_format: str) -> Tuple[str, float, str, bool]:
    """
    Parses the given string pattern to split into index format, file size, size units, 
    and zip option.

    Parameters:
    - task_splitindex_format: A string pattern in the 
    format `{index}|size|zip_option`, e.g., `{_index}|500MB|NOZIP`.

    Returns:
    - A tuple with four elements: (index_format, max_file_size, size_units, zip_nozip).
    """
    # Split the string using the pipe character
    parts = task_splitindex_format.split('|')

    if len(parts) != 3:
        raise ValueError(
            "Input string does not match the expected pattern '{index}|size|zip_option'.")

    # Extract components from the split parts
    index_format: str = parts[0]   # The index format
    max_file_size_str: str = parts[1].upper()
    zip_option_str: str = parts[2].upper()

    # Validate zip option and convert to boolean
    zip_nozip: bool
    if zip_option_str == "ZIPFILES":
        zip_nozip = True
    elif zip_option_str == "NOZIP":
        zip_nozip = False
    else:
        raise ValueError("Zip Files component must be 'ZIPFILES' or 'NOZIP'.")

    # Use regex to separate numeric file size and units (allow decimals)
    size_pattern = r'(\d+(?:\.\d+)?)(KB|MB|GB)'
    match = re.match(size_pattern, max_file_size_str)

    if not match:
        raise ValueError(
            "File size component does not match the expected format like '500MB' or '300.5KB'.")

    # Convert the numeric value to a float
    max_file_size: float = float(match.group(1))
    size_units: str = match.group(2)  # Extract the size units

    return index_format, max_file_size, size_units, zip_nozip


def prep_and_copy_to_bucket(
    file_list: List[str],
    task_grp_nm: str,
    destination_bucket: str,
    destination_path: str,
    wwconfig_path: str,
    gcp_project_name : str
) -> List[str]:
    """
        file_list          list: A list of filepaths to be copied.
        task_grp_nm         str: TASK_GRP_NM from configuration table [dbo].[ACONFIG_PROCESNG_RULES]
        source_path         str: Local path or network share.
        file_pattern        str: File name ("NH_HB_AdmissionSource_20250305.txt") or a pattern 
                                 of files to be copied ("NH_HB_*.txt")
        destination_bucket  str: A valid GCP bucket name
        destination_path    str: A valid folder path inside the GCP bucket
    """

    # Validate and prepare parameters to copy file to GCP bucket
    bucket_name: str
    if is_none_or_empty(destination_bucket):
        # Raise an error if the destination bucket is not provided
        raise ValueError("Destination bucket is required.")
    else:
        bucket_name = destination_bucket

    # Prepare the folder name to be used inside the GCP bucket
    UNWANTED_PART = r'\\nasdata204\sharedata\HealthcareAnalyticSecureTransfers-PHI\Epic_Extracts' \
        + '\\'

    final_destination_folder = destination_path.replace(
        UNWANTED_PART, "").replace('\\', '/')

    # DEBUG CODE
    print('destination_path = ', destination_path)
    print('UNWANTED_PART = ', UNWANTED_PART)
    print("final_destination_folder = ", final_destination_folder)

    copied_files: List[str] = []

    if not isinstance(file_list, list) or not file_list:
        return copied_files  # Return empty list if not a valid list or is empty

    # Traverse list of files to prepare their necessary parameters to copy them to a GCP bucket
    for file_path in file_list:
        try:
            # Separate the components of the source path
            folder_path = os.path.dirname(file_path)
            file_name_with_ext = os.path.basename(file_path)
            file_name, file_ext = os.path.splitext(file_name_with_ext)

            # set up logging
            log_file_name = task_grp_nm + "_gcp_copy.log"
            log_full_path = os.path.join(folder_path, log_file_name)
            setup_logging(log_file=log_full_path, console_output=True)

            # Copy file(s) to GCP bucket
            copy_files_to_gcp(source_path=folder_path,
                              pattern=file_name_with_ext,
                              bucket_name=bucket_name,
                              destination_folder=final_destination_folder,
                              wwconfig_path=wwconfig_path,
                              gcp_project_name=gcp_project_name
                              )

            # Add the returned destination URI to the list of copied files.
            copied_files.append(file_path)

        except Exception as e:  # Catch and re-raise any exceptions from prep_and_copy_to_bucket
            print(f"Error copying file {file_path} to GCP: {e}")
            raise

    return copied_files


def convert_size_to_bytes(size, unit):
    """Converts file size with unit to bytes."""
    unit = unit.upper()
    if unit == "KB":
        return size * 1024
    elif unit == "MB":
        return size * 1024 * 1024
    elif unit == "GB":
        return size * 1024 * 1024 * 1024
    else:
        raise ValueError("Invalid size unit. Use KB, MB, or GB.")


def rename_tmp_file(temp_filename: str, filename: str):
    # check if the target filename already exists in the folder, if so
    #  delete it to replace it with the new file
    if os.path.exists(filename):
        try:
            print(f"Deleting existing file '{filename}'.")
            os.remove(filename)
            print(f"Existing file '{filename}' deleted successfully.")
        except OSError as e:
            print(f"Error deleting file '{filename}': {e}")
            raise
    else:
        print(f"File '{filename}' does not exist.")

    # Rename the temp file to the proper file name
    try:
        print(f"Renaming file '{temp_filename}' to '{filename}'.")
        os.rename(temp_filename, filename)
        print(f"File '{temp_filename}' renamed to '{filename}' successfully.")
    except OSError as e:
        print(f"Error renaming file: {e}")
        raise


def write_data_to_multiple_files(
    cursor,
    filename: str,
    delimiter: str = ',',
    include_headers: bool = True,
    quoting=csv.QUOTE_MINIMAL,
    force_single_file=True,
    batch_size: int = 1000,
    max_file_size: float = 1.95,
    size_unit: str = 'GB',
    index_format: str = "_{index:01d}"
):
    """
    Writes data from an SQL Server cursor to a single or multiple delimited
    text files based on the force_single_file parameter. Uses a .tmp extension
    for the single file while writing and renames it at the end.

    Parameters:
        cursor (pyodbc.Cursor): A pre-executed pyodbc cursor object ready to fetch data.
        filename (str): The base filename for the output file(s).  The extension will be extracted.
        max_file_size (float): The maximum size of each output file in multi-file mode (ignored 
                               if force_single_file is True).
        size_unit (str): The unit for max_file_size (KB, MB, GB).  
                         Ignored if force_single_file is True.
        index_format (str, optional): The format string for the file index in multi-file mode. 
                                      Defaults to "_{index:01d}". Ignored if force_single_file is True.
        delimiter (str, optional): The delimiter character used in the output file(s). 
                                   Defaults to ','.
        include_headers (bool, optional): Whether to include column headers in the output file(s). 
                                          Defaults to True.
        quoting (int, optional): Quoting option for csv.writer 
                                 (e.g., csv.QUOTE_MINIMAL, csv.QUOTE_ALL). 
                                 Defaults to csv.QUOTE_MINIMAL.
        batch_size (int, optional): Number of database rows to fetch at a time. Defaults to 1000.
        force_single_file (bool, optional): If True, writes all data to a single file. If False, 
                                            uses multi-file mode. Defaults to True.

    Returns:
        tuple: A tuple containing 
               (total_rows_read, total_rows_written, errors_encountered, generated_files).
               generated_files is a list of the filenames created.

    Raises:
        ValueError: If the total rows read do not match the total rows written.
        BusinessTaskError: If cursor has no result set.
    """

    operation_context: str = ''

    total_rows_read: int = 0
    total_rows_written: int = 0
    errors_encountered: int = 0
    generated_files: List[str] = []
    column_names: List[str] = []

    # *** CRITICAL FIX: Check if cursor has a result set ***
    if cursor.description is None:
        error_msg = (
            "Cursor has no result set (description is None). "
            "The query must return a SELECT statement with data. "
            "Check if your stored procedure or SQL query includes a SELECT statement."
        )
        raise BusinessTaskError(
            error_msg,
            execution_context={"operation": "Validating cursor has result set", "filename": filename}
        )

    # utility function to write a batch of data to a text file
    def write_batch_to_file(writer, rows):
        """Helper function to write rows and update total_rows_written *once*."""
        nonlocal total_rows_written
        writer.writerows(rows)  # Write all rows at once
        total_rows_written += len(rows)  # Increment only once per batch

    temp_filename: str = ''
    final_filename: str = ''

    if force_single_file:  # Single file mode
        base_filename, base_ext = os.path.splitext(filename)
        temp_filename = f"{base_filename}.tmp"

        try:
            operation_context = f"Creating temporary file in write mode '{temp_filename}'"

            with open(temp_filename, mode='w', newline='', encoding='cp1252') as file:

                writer = csv.writer(file, delimiter=delimiter,
                                    quotechar='"', quoting=quoting)
                if include_headers:
                    # Extra safety check
                    if cursor.description is None:
                        raise ValueError("Cursor description became None during processing")
                    
                    column_names = [column[0] for column in cursor.description]

                    operation_context = "Writing file header to temporary file"
                    writer.writerow(column_names)

                while True:
                    operation_context = "Fetching data rows from cursor"
                    rows = cursor.fetchmany(batch_size)
                    total_rows_read += len(rows)
                    if not rows:
                        break
                    operation_context = "Writing data row to temporary file"
                    write_batch_to_file(writer, rows)

        except IOError as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context,
                "temp_filename": temp_filename
            }
            raise BusinessTaskError(
                "IO error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except (pyodbc.Error) as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Database error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except Exception as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Unexpected error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex

        try:
            operation_context = f"Renaming temp file '{temp_filename}' to the expected file name '{filename}'"
            rename_tmp_file(temp_filename, filename)

            operation_context = "Adding newly-generated file name to the list"
            generated_files.append(filename)

        except IOError as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "IO error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except Exception as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Unexpected error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex

    else:  # Multi-file mode

        max_bytes = convert_size_to_bytes(max_file_size, size_unit)
        file_index = 1
        base_filename, base_ext = os.path.splitext(filename)

        current_file_size: float = 0.0
        remaining_rows = []  # Store rows that didn't fit in the previous file

        if include_headers:
            # Extra safety check
            if cursor.description is None:
                raise ValueError("Cursor description is None - cannot retrieve column names")
            
            column_names = [column[0] for column in cursor.description]

        try:

            operation_context = "Reading the first batch of data from the cursor"
            rows = cursor.fetchmany(batch_size)
            total_rows_read += len(rows)

            # if there are no rows in the cursor, no files need to be created
            if total_rows_read == 0:
                print("No rows were returned from the cursor. No files will be created.")
            else:

                # begin with all the rows read in the initial batch
                remaining_rows = rows
                header_size: float
                accumulated_size: float

                while True:

                    # build the temporary and final file names for the file to be generated
                    formatted_index = calc_seq_index(index_format, file_index)
                    temp_filename = f"{base_filename.replace('{SPLITINDEX}', formatted_index)}.tmp"
                    final_filename = f"{base_filename.replace('{SPLITINDEX}', formatted_index)}{base_ext}"

                    operation_context = f"Creating a new output file '{temp_filename}'"
                    with open(temp_filename, mode='w', newline='', encoding='cp1252') as file:
                        writer = csv.writer(
                            file, delimiter=delimiter, quotechar='"', quoting=quoting)

                        current_file_size = 0.0

                        if include_headers:
                            operation_context = f"Writing headers to output file '{temp_filename}'"
                            writer.writerow(column_names)
                            header_size = sum(len(str(
                                name)) for name in column_names) + len(delimiter) * (len(column_names) - 1) + 2
                            current_file_size += header_size

                        while True:  # Process remaining_rows and new batches

                            if remaining_rows:  # Write any leftover rows from previous file first
                                string_buffer = io.StringIO()
                                temp_writer = csv.writer(
                                    string_buffer, delimiter=delimiter, quotechar='"', quoting=quoting)
                                rows_to_write = []
                                accumulated_size = 0.0
                                for row in remaining_rows:
                                    row_string = io.StringIO()
                                    temp_row_writer = csv.writer(
                                        row_string, delimiter=delimiter, quotechar='"', quoting=quoting)
                                    temp_row_writer.writerow(row)
                                    row_str = row_string.getvalue()
                                    row_size = len(row_str.encode('cp1252'))
                                    if accumulated_size + row_size < max_bytes - current_file_size:
                                        accumulated_size += row_size
                                        rows_to_write.append(row)
                                    else:
                                        break    # Innermost loop to write the rows that fit

                                operation_context = "Writing remaining rows to file"
                                write_batch_to_file(writer, rows_to_write)
                                current_file_size += accumulated_size

                                # Remove only the rows that were written
                                remaining_rows = remaining_rows[len(rows_to_write):]
                                if not remaining_rows:
                                    remaining_rows = []  # Reset remaining_rows after processing

                            if not remaining_rows:  # Only fetch if remaining_rows is empty
                                rows = cursor.fetchmany(batch_size)
                                total_rows_read += len(rows)

                                if not rows:  # No more rows from cursor
                                    if current_file_size > 0:
                                        file.flush()
                                        file.close()

                                        operation_context = f"Renaming temp file '{temp_filename}' to '{final_filename}'"
                                        rename_tmp_file(temp_filename, final_filename)

                                        # add newly-generated file name to the list
                                        generated_files.append(final_filename)

                                    break  # Exit inner loop (no more rows)

                                string_buffer = io.StringIO()
                                temp_writer = csv.writer(
                                    string_buffer, delimiter=delimiter, quotechar='"', quoting=quoting)
                                temp_writer.writerows(rows)
                                batch_string = string_buffer.getvalue()
                                batch_size_bytes = len(batch_string.encode('cp1252'))  # Fixed to cp1252

                                if current_file_size + batch_size_bytes < max_bytes:
                                    current_file_size += batch_size_bytes
                                    file.write(batch_string)
                                    total_rows_written += len(rows)
                                else:  # Time to create a new file
                                    remaining_rows = rows

                                    file.flush()
                                    file.close()

                                    operation_context = f"Renaming temp file '{temp_filename}' to '{final_filename}'"
                                    rename_tmp_file(temp_filename, final_filename)

                                    # add newly-generated file name to the list
                                    generated_files.append(final_filename)

                                    break  # Exit inner loop to create a new file due to size limit

                            elif remaining_rows:  # if there are still remaining rows from the previous file

                                file.flush()
                                file.close()

                                operation_context = f"Renaming temp file '{temp_filename}' to '{final_filename}'"
                                rename_tmp_file(temp_filename, final_filename)

                                # add newly-generated file name to the list
                                generated_files.append(final_filename)

                                break

                    if not rows:  # No more rows to fetch
                        break  # Exit the outer loop when there are no more rows to fetch
                    file_index += 1

        except IOError as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "IO error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except (pyodbc.Error) as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Database error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex
        except Exception as ex:
            errors_encountered += 1
            context = {
                "operation": operation_context
            }
            raise BusinessTaskError(
                "Unexpected error occurred",
                execution_context=context,
                original_exception=ex
            ) from ex

    if total_rows_read != total_rows_written:
        raise ValueError(
            f"Row count mismatch: read {total_rows_read}, but wrote {total_rows_written}.")

    return total_rows_read, total_rows_written, errors_encountered, generated_files


def copy_files_to_destination(generated_files, destination_folder, create_destination=False):
    """
    Copies a list of files to a destination folder.

    Parameters:
        generated_files (list): A list of filepaths to be copied.
        destination_folder (str): The destination folder path.
        create_destination (bool, optional): If True, creates the destination 
                                            folder if it doesn't exist. 
                                            If False, raises an error if the 
                                            destination folder doesn't exist. 
                                            Defaults to False.

    Returns:
        list: A list of the filepaths of the copied files in the 
              destination folder. Returns an empty list if the 
              generated_files parameter is an empty list, or if it
              is not a list.

    Raises:
        FileNotFoundError: If a source file does not exist.
        IOError: If there are other issues during copying.
        ValueError: If the destination folder does not exist and 
                    create_destination is False.

    """
    copied_files = []

    if not isinstance(generated_files, list) or not generated_files:
        # Return empty list if not a valid list or is empty.
        return copied_files

    if not create_destination:  # Raise error if destination folder does not exist
        if not os.path.exists(destination_folder):
            raise ValueError(
                f"Destination folder '{destination_folder}' does not exist.")
    else:
        os.makedirs(destination_folder, exist_ok=True)

    for file_path in generated_files:
        try:
            destination_path = os.path.join(
                destination_folder, os.path.basename(file_path))
            shutil.copy2(file_path, destination_path)
            copied_files.append(destination_path)

        except FileNotFoundError:
            print(f"Error: Source file not found: {file_path}")
            raise  # Re-raise the exception
        except IOError as e:
            print(f"Error copying file {file_path}: {e}")
            raise  # Re-raise to stop if copy fails

    return copied_files


def str2bool(v) -> bool:
    """Convert a string to a boolean value."""
    if isinstance(v, bool):
        return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def validate_and_start_extract() -> None:
    """
    Produce a data extract for a given Task Name
    """
    operation_context: str = ''

    try:

        # Define the command-line arguments expected by the script
        parser = argparse.ArgumentParser(description="Process script parameters.")
        parser.add_argument("bi_server", type=str, help="BI SQL Server name")
        parser.add_argument("bi_database", type=str, help="BI Database name")
        parser.add_argument("task_name", type=str, help="Task name")
        parser.add_argument("task_process_step_id", type=int, help="Task process step ID")
        parser.add_argument("copy_files_to_archive", nargs='?',type=str2bool, default=False, help="Copy files to archive")
        parser.add_argument("copy_files_to_gcp_bucket", nargs='?',type=str2bool, default=False, help="Copy files to GCP bucket")
        parser.add_argument("wwconfig_path", nargs='?',type=str, default="", help="Path to the Workload Identity Federation config file")

        args = parser.parse_args()

        # Accessing mandatory positional command-line arguments
        ConnEpicBIServer = args.bi_server
        ConnEpicBIDatabase = args.bi_database
        TaskName = args.task_name
        TaskProcessStepID = args.task_process_step_id
        wwconfig_path: str = ""

        # Print mandatory positional command-line arguments
        print(f"BI Server: {ConnEpicBIServer}")
        print(f"BI Database: {ConnEpicBIDatabase}")
        print(f"Task Name: {TaskName}")
        print(f"Task Process Step ID: {TaskProcessStepID}")

        # Accessing optional command-line arguments
        if args.copy_files_to_archive:
            copy_files_to_archive = args.copy_files_to_archive
        else:
            copy_files_to_archive = False

        if args.copy_files_to_gcp_bucket:
            copy_files_to_gcp_bucket = args.copy_files_to_gcp_bucket

            # If copying files to GCP bucket is enabled, then validate the WIF config path
            wwconfig_path = args.wwconfig_path
            if is_none_or_empty(wwconfig_path):
                # Raise an error if the WIF config path is not provided
                raise ValueError("Path to the Workload Identity Federation config file is required.")
        else:
            copy_files_to_gcp_bucket = False

        # Print optional positional command-line arguments
        print(f"Copy files to archive: {copy_files_to_archive}")
        print(f"Copy files to GCP bucket: {copy_files_to_gcp_bucket}")
        print(f"WIF config path: {wwconfig_path}")

        # Create a connection string for BI SQL Server
        ConnectionStringEpicBI = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + \
            ConnEpicBIServer + ';DATABASE='+ ConnEpicBIDatabase + \
            ';Trusted_Connection=yes;TrustServerCertificate=yes'
        print(f"Connection string to BI Server: {ConnectionStringEpicBI}")
        ConnEpicBI = pyodbc.connect(ConnectionStringEpicBI)
        ConnEpicBI.autocommit = True
        cursorEpicBI = ConnEpicBI.cursor()
        cursorEpicBIlog = ConnEpicBI.cursor()

        # Initialize counters and names
        source_record_count: int = 0
        target_record_count: int = 0
        file_part_name = ''
        date_part_name = ''
        TargetName = None

        # Read Task related Data from the Table
        cursorEpicBI.execute("SELECT TASK_ID,TASK_NM,EXTRCT_FILE_NM_FORMAT,EXTRCT_FILE_DELIMETR_TYPE,TASK_INCRMNTL_FLAG,TASK_DESTINATION_TYPE,EXTRCT_DESTINATION_PATH,EXTRCT_ARCHIVE_PATH"
                             + ",TASK_ENV_TYPE_NAME,	TASK_DB_NM,	TASK_SERVER_NM,	TASK_PRIORITY_NBR,	TASK_CODE_TYPE,	TASK_CODE,TASK_GRP_ID"
                             + ",DESTINATION_SERVER_NM ,DESTINATION_DB_NM ,DESTINATION_TBL_NM,DESTINATION_TBL_TRUNC_FLAG,TASK_PARAMS,FORMAT_FILE_TYPES "
                             + ",TASK_SPLITINDEX_FORMAT, TASK_GRP_NM, EXTRCT_DESTINATION_GCP_BUCKET, EXTRCT_DESTINATION_GCP_PROJECT "
                             + ",EXTRCT_COPY_DESTINATION_PATH, EXTRCT_ZIP_TYPE, EXTRCT_ZIP_FILE_NM_FORMAT "
                             + " FROM [dbo].[ACONFIG_PROCESNG_RULES]"
                             + " WHERE 1=1 "
                             + " AND TASK_ACTV_FLG =1"
                             + " AND TASK_NM ='"+TaskName + "'")
        TaskData = cursorEpicBI.fetchone()
        Extract_File_NM_Format = TaskData[2]
        Extract_File_Delimeter = TaskData[3]
        Task_Incrmntl_Flag = TaskData[4]
        Task_Dest_Type = TaskData[5]
        File_Dest_Path = TaskData[6]
        File_Archive_Path = TaskData[7]
        Task_Env_Type_NM = TaskData[8]
        Task_DB_NM = TaskData[9]
        Task_Server_NM = TaskData[10]
        Task_Code_Type = TaskData[12]
        Task_CodeQuery = TaskData[13]
        Task_Grp_Id = TaskData[14]
        Destn_Server_Nm = TaskData[15]
        Destn_DB_Nm = TaskData[16]
        Destn_TBL_NM = TaskData[17]
        Dest_TBL_Trunct_Flag = TaskData[18]
        Task_param = TaskData[19]
        FORMAT_FILE_TYPES = TaskData[20]
        task_splitindex_format = TaskData[21]
        task_grp_nm = TaskData[22]
        extrct_destination_gcp_bucket = TaskData[23]
        extrct_destination_gcp_project = TaskData[24]
        extrct_copy_destination_path = TaskData[25]
        extrct_zip_type = TaskData[26]
        extrct_zip_file_nm_format = TaskData[27]

        Include_Header = bool(1)

        # Determine whether to include header
        Include_Header = bool(0) if (not is_none_or_empty(FORMAT_FILE_TYPES)) and ("NO_HEADER" in FORMAT_FILE_TYPES.upper()) else bool(1)

        # Determine whether to quote fields
        ADD_Quote = csv.QUOTE_ALL if (not is_none_or_empty(FORMAT_FILE_TYPES)) and ("ADD_QUOTE" in FORMAT_FILE_TYPES.upper()) else csv.QUOTE_MINIMAL

        # From the Audit Tables get the incremental Date from the last successful process
        cursorEpicBI.execute("SELECT ISNULL(LAST_INCRMNTL_DT,'1900-01-01') AS LAST_INCRMNTL_DT FROM AAUDIT_TBL_LOG (NOLOCK)"
                             + " WHERE PRCS_ID in ( SELECT MAX(PRCS_ID) FROM APROCESS_TBL_LOG (NOLOCK) "
                             + " WHERE 1=1 AND PRCS_STAT ='SUCCESSFUL' "
                             + " AND TASK_GRP_ID ='"+Task_Grp_Id + "')"
                             + " AND PRCS_TASK_NM ='"+TaskName + "'")

        # Get first row from Audit table
        LastIncrementalRow = cursorEpicBI.fetchone()

        # Validate if there is a value else default to '1900-01-01'
        if LastIncrementalRow is None:
            LastIncrementalDate = datetime(1900, 1, 1).date()
            LastIncrementalDate = LastIncrementalDate.strftime("%Y-%m-%d")
        else:
            LastIncrementalDate = LastIncrementalRow[0]

        print("LastIncrementalDate: ",LastIncrementalDate)

        # modify the code based on if its a SQL Query or a Stored Procedure
        if Task_Code_Type == 'SQL':
            Task_CodeQuery = Task_CodeQuery
        elif Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 0:
            Task_CodeQuery = "EXEC " + Task_CodeQuery
        elif Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
            Task_CodeQuery = " DECLARE @output_param DATE; EXEC " + Task_CodeQuery + \
                " @INputDate = ?, @outputDate = @output_param OUTPUT; SELECT @output_param AS out_param_value;"
        elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param):
            Task_CodeQuery = Task_CodeQuery
        print("Task_CodeQuery: ",Task_CodeQuery)

        # Connect to Task code SQL Server using Windows auth
        ConnectStringTaskCode = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + \
            Task_Server_NM+';DATABASE='+Task_DB_NM + \
            ';Trusted_Connection=yes;TrustServerCertificate=yes'
        ConnTaskCode = pyodbc.connect(ConnectStringTaskCode)
        ConnTaskCode.autocommit = True
        cursorTaskCode = ConnTaskCode.cursor()

        print("Task_param: ", Task_param)

        if not is_none_or_empty(Task_param):

            try:
                operation_context = "Executing query to fetch Task Parameters"
                cursorTaskCode.execute(Task_param)
            except Exception as ex:
                context = {
                    "operation": operation_context
                }
                raise BusinessTaskError(
                    "Unexpected error occurred while executing query to fetch Task Parameters",
                    execution_context=context,
                    original_exception=ex
                ) from ex

            try:
                operation_context = "Fetching all rows from query to fetch Task Parameters"
                Task_param_rows = cursorTaskCode.fetchall()
            except Exception as ex:
                context = {
                    "operation": operation_context
                }
                raise BusinessTaskError(
                    "Unexpected error occurred while fetching all rows from query to fetch Task Parameters",
                    execution_context=context,
                    original_exception=ex
                ) from ex

            TaskParam_names = []  # Initialize an empty list to store parameter names

            print("Task_param_rows length: ", len(Task_param_rows))
            if len(Task_param_rows) == 0:
                print("\tWARNING: No rows found for Task_param.")
                print("\tTask_param: ", Task_param)
                print("\tInvestigate why no rows were found for Task_param.")

            # Get column names
            TaskParamColumn_names = [column[0]
                                     for column in cursorTaskCode.description]

            # Find the filename and date columns
            filename_column = next(
                (col for col in TaskParamColumn_names if 'file_name_part' in col.lower()), None)
            date_column = next(
                (col for col in TaskParamColumn_names if 'date_part' in col.lower()), None)

            for i in cursorTaskCode.description:
                TaskParam_name = f"@{i[0]}"
                # Add the parameter name to the param_names list
                TaskParam_names.append(TaskParam_name)

            if filename_column and date_column:
                TaskParam_names = TaskParam_names[:-2]
            elif filename_column or date_column:
                TaskParam_names = TaskParam_names[:-1]
            else:
                TaskParam_names = TaskParam_names[:]

                # Construct the SQL query with parameter placeholders
            Taskparam_placeholders = ', '.join(
                f"{ParamName}=?" for ParamName in TaskParam_names)
            Task_CodeQuery = f"{Task_CodeQuery} {Taskparam_placeholders}"
            print("Task_CodeQuery: ", Task_CodeQuery)

        else:
            Task_param_rows = [(1, "NAME")]

        for TaskParamRecords in Task_param_rows:

            TaskParams_values = []  # Initialize an empty list to store parameter values

            if not is_none_or_empty(Task_param):
                if filename_column and date_column:

                    # Excluding the file name and date column
                    Taskparams = TaskParamRecords[:-2]
                    # Get the file name from the filename column
                    file_part_name = TaskParamRecords[-2]
                    date_part_name = TaskParamRecords[-1]
                    if len(TaskParamRecords) > 2:
                        # Excluding the file name or date column
                        Taskparams = TaskParamRecords[:-2]
                        TaskParamRecords = TaskParamRecords[:-2]
                    else:
                        Taskparams = []
                        TaskParamRecords = []

                elif filename_column or date_column:
                    if filename_column:
                        file_part_name = TaskParamRecords[-1]
                    if date_column:
                        date_part_name = TaskParamRecords[-1]
                    if len(TaskParamRecords) > 1:
                        # Excluding the file name or date column
                        Taskparams = TaskParamRecords[:-1]
                        TaskParamRecords = TaskParamRecords[:-1]
                    else:
                        Taskparams = []
                        TaskParamRecords = []

                else:
                    Taskparams = TaskParamRecords[:]

            # Iterate over the columns in the row
            for i, value in enumerate(TaskParamRecords[:], start=1):
                # Add the value to the params list
                TaskParams_values.append(value)

            # Execute Task Code query
            if Task_Code_Type == 'SP' and is_none_or_empty(Task_param) and Task_Incrmntl_Flag == 1:
                params = (LastIncrementalDate,)
                cursorTaskCode.execute(Task_CodeQuery, params)
            elif Task_Code_Type == 'SP' and not is_none_or_empty(Task_param):
                print(f"TaskParams_values: {TaskParams_values}")
                print(f"Task_CodeQuery: {Task_CodeQuery}")
                # Execute the stored procedure with dynamic parameters
                cursorTaskCode.execute(Task_CodeQuery, TaskParams_values)
            else:
                cursorTaskCode.execute(Task_CodeQuery)

            # *** CRITICAL FIX: Check cursor status after query execution ***
            print("\n=== CHECKING CURSOR STATUS ===")
            print(f"Cursor description: {cursorTaskCode.description}")

            # Check if cursor has a result set
            if cursorTaskCode.description is None:
                print("WARNING: Cursor description is None!")
                print(f"Task Code Type: {Task_Code_Type}")
                print(f"Query executed: {Task_CodeQuery}")
                
                # Try to navigate to next result set (for stored procedures with multiple result sets)
                print("Attempting to find a valid result set...")
                has_valid_resultset = False
                resultset_count = 0
                
                while cursorTaskCode.nextset():
                    resultset_count += 1
                    print(f"Found result set #{resultset_count}")
                    if cursorTaskCode.description is not None:
                        print(f"  - Has {len(cursorTaskCode.description)} columns")
                        has_valid_resultset = True
                        break
                    else:
                        print(f"  - Result set #{resultset_count} has no columns (description is None)")
                
                if not has_valid_resultset:
                    error_msg = (
                        f"Query did not return a valid result set with data. "
                        f"Checked {resultset_count + 1} result sets. "
                        f"Ensure your stored procedure or SQL query includes a SELECT statement that returns data."
                    )
                    raise BusinessTaskError(
                        error_msg,
                        execution_context={
                            "Task_Code_Type": Task_Code_Type,
                            "Task_CodeQuery": Task_CodeQuery,
                            "Task_Name": TaskName
                        }
                    )
                else:
                    print(f"Successfully found valid result set with {len(cursorTaskCode.description)} columns")
            else:
                print(f"Cursor has {len(cursorTaskCode.description)} columns")
                print(f"Column names: {[col[0] for col in cursorTaskCode.description]}")

            print("=== END CURSOR STATUS CHECK ===\n")

            # Evaluate if the Destination is a file and create file else skip to next step
            if Task_Dest_Type.upper() == "FILE":

                now = datetime.now()

                # Find last underscore index
                last_idx = Extract_File_NM_Format.rindex('_')
                # Get substring after last underscore
                FileNMDateSubstr = Extract_File_NM_Format[last_idx+1:]
                # Get File Date Format
                FileDateFormat = FileNMDateSubstr.split(".")[0]

                if FileDateFormat.upper() == 'YYYYMMDD':
                    FileDateFormatString = '%Y%m%d'
                elif FileDateFormat.upper() == 'MMDDYYYY':
                    FileDateFormatString = '%m%d%Y'
                else:
                    FileDateFormatString = FileDateFormat

                FileDateFormat_wo_SplitIndex = FileDateFormat.replace("{SPLITINDEX}", "")

                if FileDateFormat_wo_SplitIndex.upper() == 'YYYYMMDD':
                    FileDateFormatString = '%Y%m%d'
                elif FileDateFormat_wo_SplitIndex.upper() == 'MMDDYYYY':
                    FileDateFormatString = '%m%d%Y'
                else:
                    FileDateFormatString = FileDateFormat_wo_SplitIndex

                # Validate date
                try:
                    # Get today's date
                    current_date = datetime.now()
                    # First, parse the input string into a datetime object
                    DateFormat = current_date.strftime(FileDateFormatString)

                    formatted_date = DateFormat
                except ValueError:
                    # Invalid date, use default
                    DateFormat = '%Y%m%d'
                    # Format the date
                    formatted_date = now.strftime(DateFormat)

                    # Construct file name with date format

                Extract_File_NM = Extract_File_NM_Format.replace(
                    FileDateFormat_wo_SplitIndex, formatted_date)

                Extract_File_NM = Extract_File_NM.replace(
                    "<FILENAMEPART>", file_part_name)
                Extract_File_NM = Extract_File_NM.replace(
                    "<DATEPART>", date_part_name)

                TargetName = Extract_File_NM
                output_file = f'{File_Dest_Path}\\{Extract_File_NM}'
                archive_file_name: str = Extract_File_NM.replace("{SPLITINDEX}", "")
                Archive_Path = f'{File_Archive_Path}\\{archive_file_name}'

                print("output_file = ", output_file)
                print("Archive_Path = ", Archive_Path)

                # Read the data from the DB in small batches (N rows at a time) and write it to
                # the file to avoid memory overflow
                batch_size: int = 5000
                force_single_file: bool

                # Define the return variables after writing the data to the file(s)
                errors_encountered: int = 0
                generated_files: List[str] = []

                # Define variables used for multiple file generation and zipping
                file_seq_format: str
                max_file_size: float
                max_size_units: str
                zip_nozip: bool

                # Check if the extract requires to export the data to a single file or
                # to multiple files
                if is_none_or_empty(task_splitindex_format):
                    force_single_file = True
                    file_seq_format = ''
                    max_file_size = 50.0
                    max_size_units = 'GB'
                    zip_nozip = False
                else:
                    # The current extract requires export the data into multiple files
                    force_single_file = False

                    # Parse out the task_splitindex_format.
                    file_seq_format, max_file_size, max_size_units, zip_nozip = parse_task_splitindex_format(
                        task_splitindex_format)

                # Export the data to one or multiple files
                try:
                    operation_context = f"Invoking method to export data to file '{output_file}'"
                    source_record_count, target_record_count, errors_encountered, generated_files = write_data_to_multiple_files(
                        cursor=cursorTaskCode,
                        filename=output_file,
                        delimiter=Extract_File_Delimeter,
                        include_headers=Include_Header,
                        quoting=ADD_Quote,
                        force_single_file=force_single_file,
                        batch_size=batch_size,
                        max_file_size=max_file_size,
                        size_unit=max_size_units,
                        index_format=file_seq_format)
                except Exception as ex:
                    context = {
                        "operation": operation_context
                    }
                    raise BusinessTaskError(
                        "Unexpected error occurred while executing write_data_to_multiple_files method",
                        execution_context=context,
                        original_exception=ex
                    ) from ex

                print(f"Total Errors: {errors_encountered}")
                print("source_record_count = ", source_record_count)
                print("target_record_count = ", target_record_count)

                print(f"File exported to {output_file} successfully!")

                print(f"File part name: {file_part_name}")
                print(f"Date part name: {date_part_name}")

                print(f"Task Incremental Flag: {Task_Incrmntl_Flag}")

                print(f"Extract Zip Type: {extrct_zip_type}")
                print(f"Extract Zip File Name Format: {extrct_zip_file_nm_format}")

                # If necessary, perform zipping of the generated files
                if not is_none_or_empty(extrct_zip_type):
                    if extrct_zip_type.upper() == 'ZIP_GROUP':
                        try:
                            zipping_log_file_path = os.path.join(
                                os.path.dirname(output_file),
                                f"{task_grp_nm}_zipping.log"
                            )

                            zip_file_path = append_zip_from_files(
                                file_list=generated_files,
                                zip_file_name_format=extrct_zip_file_nm_format,
                                log_file_path=zipping_log_file_path
                            )
                            print("List of files successfully zipped to: ", zip_file_path)

                            if zip_file_path:
                                generated_files.clear()

                            generated_files.append(zip_file_path)

                        except (FileNotFoundError, IOError):
                            print("Error during file list zipping operation.")
                        except Exception as e:
                            print("Error occurred while zipping files.")
                            print(e)
                else:
                    print("No file zipping type provided. Therefore, no zipping operation performed.")

                # Copy files to extra destination if configured
                if not is_none_or_empty(extrct_copy_destination_path):
                    if os.path.exists(extrct_copy_destination_path):
                        try:
                            copy_files_to_destination(
                                generated_files=generated_files,
                                destination_folder=extrct_copy_destination_path,
                                create_destination=True
                            )
                            print("Extra copies of files successfully copied to: ", extrct_copy_destination_path)
                        except (FileNotFoundError, IOError):
                            print("Error during file copy operation. Check source files and destination folder.")
                else:
                    print("No extraction copy destination path provided. Therefore, no extra copy operation performed.")

                # Based on command-line argument copy the generated file(s) to the Archive folder
                if copy_files_to_archive:
                    copied_file_list: List[str] = []
                    try:
                        copied_file_list = copy_files_to_destination(
                            generated_files=generated_files,
                            destination_folder=File_Archive_Path,
                            create_destination=False)
                        if copied_file_list:
                            print("Files successfully copied to:", File_Archive_Path)
                            print("Copied files:", copied_file_list)
                        else:
                            print("No files copied to destination.")

                    except (FileNotFoundError, IOError):
                        print("Error during file copy operation.  Check source files and destination folder.")
                        raise

                if copy_files_to_gcp_bucket:
                    copied_files_to_bucket: List[str] = []

                    try:
                        copied_files_to_bucket = prep_and_copy_to_bucket(
                                file_list=generated_files,
                                task_grp_nm=task_grp_nm,
                                destination_bucket=extrct_destination_gcp_bucket,
                                destination_path=File_Dest_Path,
                                wwconfig_path=wwconfig_path,
                                gcp_project_name=extrct_destination_gcp_project
                        )

                        print('Copied files to GCP bucket:')
                        print(copied_files_to_bucket)

                    except Exception as e:
                        print("Error occurred while copying files to GCP bucket:", extrct_destination_gcp_bucket)
                        print(e)

            elif Task_Dest_Type.upper() == "TABLE":

                TargetName = Destn_TBL_NM
                ConnectStringSQLDestn = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + \
                    Destn_Server_Nm+';DATABASE='+Destn_DB_Nm + \
                    ';Trusted_Connection=yes;TrustServerCertificate=yes'
                ConnSQLDest = pyodbc.connect(ConnectStringSQLDestn)
                ConnSQLDest.autocommit = True
                cursorSQLDest = ConnSQLDest.cursor()

                if Dest_TBL_Trunct_Flag == 1:
                    DestQuery = 'DELETE ' + Destn_TBL_NM
                    cursorSQLDest.execute(DestQuery)

                # Note: TaskResults is not defined in this code, this will cause an error
                # This appears to be legacy code that needs fixing
                # dest_query = "INSERT INTO " + Destn_TBL_NM + \
                #     " VALUES (" + ", ".join(["?"] * len(TaskResults[0])) + ")"

                # # Insert data into destination table
                # for SourceRow in TaskResults:
                #     cursorSQLDest.execute(dest_query, SourceRow)

                # ConnSQLDest.commit()

                # Get the number of rows in the table after insertion
                Target_count_query = f"SELECT COUNT(*) FROM {Destn_TBL_NM}"
                cursorSQLDest.execute(Target_count_query)
                target_record_count = target_record_count + \
                    cursorSQLDest.fetchone()[0]
                cursorSQLDest.close()

            else:
                print("Skipping file creation step ")

            # Get output parameter
            if cursorTaskCode.nextset() and Task_Incrmntl_Flag == 1:
                Output_Param_row = cursorTaskCode.fetchall()
                for Output_param in Output_Param_row:
                    Output_date = Output_param[0]

                update_Process_Step_sql = "UPDATE dbo.AAUDIT_TBL_LOG SET LAST_INCRMNTL_DT = ? WHERE PRCS_STEP_ID = ?"

                update_params = (Output_date, TaskProcessStepID)
                cursorEpicBIlog.execute(update_Process_Step_sql, update_params)
                print(Output_date)
                print(update_params)
                cursorEpicBIlog.commit()

        print("\nUpdate the audit log record for the current task..")
        print("TargetName = ", TargetName)

        # Update the audit log record for the current task
        update_Process_Step_SRC_TRGT = "UPDATE dbo.AAUDIT_TBL_LOG SET SRC_REC_CNT =?,TRGT_REC_CNT = ? , TRGT_NM =? WHERE PRCS_STEP_ID = ?"
        update_Process_Step_params = (
            source_record_count, target_record_count, TargetName, TaskProcessStepID)

        print("update_Process_Step_SRC_TRGT: ", update_Process_Step_SRC_TRGT)
        print("update_Process_Step_params: ", update_Process_Step_params)

        cursorEpicBIlog.execute(update_Process_Step_SRC_TRGT,
                                update_Process_Step_params)

        print("file_part_name = ", file_part_name)
        print("date_part_name = ", date_part_name)

        cursorEpicBIlog.commit()

        ConnTaskCode.close()
        ConnEpicBI.close()

        print("Code Completed")
        sys.exit()

    except Exception as e:
        # Write error to stderr
        print(f"Error: {str(e)}", file=sys.stderr)
        # Exit with non-zero status code
        sys.exit(1)


def main() -> None:
    """Entry point for the script."""
    validate_and_start_extract()


if __name__ == '__main__':
    main()
